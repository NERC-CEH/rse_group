# Pipelines

This is a potentially huge topic that could cover, at a minimum:

* Research workflows
* Data processing, storage and metadata creation
* Reproducible analytical pipelines (running models and publishing results)
* Continuous integration and deployment (building, testing and publishing research software)

The aim is not to provide a one-size-fits-all solution, but to assess what's being used in practise, aiming to consolidate on fewer choices, and develop research infrastructure that is easier to maintain and to reuse.

* Choice of open standards and open source libraries
* Observability and monitoring 
* Quality control, validation
* Provenance and metadata
* Incremental solutions which address research groups' fundamental needs while building towards a larger infrastructure

## Technologies

* [Paper summary - Pangeo Forge](pangeo_forge.md). This is an initially NSF supported, community project that provides a framework for publishing geoscience datatasets in cloud-optimised formats. It aims to help researchers focus where domain expertise is needed (preprocessing methods for analytical needs) and provide a standard set of data format transformations

* [Sustainable data analysis with SnakeMake](https://f1000research.com/articles/10-33/v2) 

* [Targets for R](https://books.ropensci.org/targets/)

* [Argo workflow engine for Kubernetes](https://argo-workflows.readthedocs.io/en/latest/)

## Next steps

* Workshop planning on specific themes (either the technologies - containerisation, environment management; or facilitating the research goals - working with cloud-optimised data formats)
* Identifying overlapping use cases
* Network building across groups within UKCEH
* Network building with open source/data communities like Pangeo Forge
* Small turn-key pieces of refactoring existing work


